{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import arpesnet as an\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Python\", sys.version)\n",
    "print(f\"Pytorch version: {torch.__version__} | GPU enabled = {\n",
    "    (torch.backends.mps.is_available() or torch.cuda.is_available())\n",
    "}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the path to the data directory\n",
    "the `root` variable should be set to the path of the directory containing the data files. it should contain:\n",
    "- test_datasets (directory containing the test images per material)\n",
    "- train_datasets (directory containing the training images per material)\n",
    "- test_imgs.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(r\"D:\\data\\ARPESdatabase\\ARPESNet\")\n",
    "assert root.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup data transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image resizing and normalization\n",
    "the `transform` variable should be set to the transformation to be applied to the images. it should contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizeAndResize = an.transform.Compose(\n",
    "    [\n",
    "        an.transform.Resize((256, 256)),\n",
    "        an.transform.NormalizeMinMax(0, 100),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## possionian noise simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100_000 # number of counts in the Poisson noise\n",
    "setExposure = an.transform.SetRandomPoissonExposure(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test images\n",
    "for visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = torch.load(root/\"test_imgs.pt\")\n",
    "test_imgs = torch.stack([normalizeAndResize(s) for s in test_imgs])\n",
    "test_imgs_noisy = torch.stack([setExposure(s) for s in test_imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, len(test_imgs), figsize=(8, 3))\n",
    "for i in range(len(test_imgs)):\n",
    "    ax[0, i].imshow(test_imgs[i].numpy(), cmap=\"viridis\", origin=\"lower\")\n",
    "    ax[1, i].imshow(test_imgs_noisy[i].numpy(), cmap=\"viridis\", origin=\"lower\")\n",
    "    counts = test_imgs_noisy[i].sum()\n",
    "    counts_per_pixel = counts / test_imgs[i].nelement()\n",
    "    ax[1, i].set_title(f\"N: {counts:,.0f}\\nN/px: {counts_per_pixel:.2f}\", fontsize=8)\n",
    "    ax[0, i].axis(\"off\")\n",
    "    ax[1, i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list((root/\"train_data\").glob(\"*.pt\"))\n",
    "train_data = torch.stack([normalizeAndResize(torch.load(f)) for f in tqdm(all_files)]).view(-1, 256, 256)\n",
    "print(f\"loaded {len(train_data):,.0f} training images with shape {train_data.shape[1:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = an.load_config(Path(an.__file__).parent.parent / \"config.yml\")\n",
    "input_shape = [256,256]\n",
    "norm = [0,100]\n",
    "config[\"model\"][\"aenc\"] = \"arpesnet\"\n",
    "config[\"model\"][\"kwargs\"] = dict(\n",
    "    kernel_size = 11,\n",
    "    kernel_decay = 2,\n",
    "    n_layers = 1,\n",
    "    start_channels = 4,\n",
    "    max_channels = 32,\n",
    "    n_blocks = 6,\n",
    "    input_shape = input_shape,\n",
    "    relu=\"PReLU\",\n",
    "    relu_kwargs=dict(num_parameters=1, init=0.25)\n",
    ")\n",
    "config[\"model\"][\"input_shape\"] = input_shape\n",
    "\n",
    "config['preprocessing']['Resize'] = input_shape\n",
    "config['preprocessing']['NormalizeMinMax'] = norm\n",
    "\n",
    "config['training_augmentations'][\"NormalizeMinMax\"] = norm\n",
    "config['training_augmentations'][\"RandomResizedCrop\"][\"size\"] = input_shape\n",
    "\n",
    "config[\"validation_augmentations\"][\"NormalizeMinMax\"] = norm\n",
    "config[\"validation_augmentations\"][\"Resize\"] = input_shape\n",
    "\n",
    "config[\"noise_augmentations\"][\"NormalizeMinMax\"] = norm\n",
    "config[\"noise_augmentations\"][\"SetRandomPoissonExposure\"] = [50_000,100_000_000]\n",
    "\n",
    "config[\"loss\"][\"criteria\"] = [\"mse\"]\n",
    "\n",
    "config[\"optimizer\"][\"name\"] = \"Adam\"\n",
    "config[\"optimizer\"][\"lr\"] = 0.001\n",
    "\n",
    "config[\"train\"][\"batch_size\"] = 32\n",
    "config[\"train\"][\"denoiser\"] = False\n",
    "config[\"train\"][\"shuffle\"] = True\n",
    "config[\"train\"][\"drop_last\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = an.ModelTrainer(config, verbose=\"full\", train_dataset=train_data)\n",
    "trainer.describe_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    n_epochs=2,\n",
    "    milestones=[2, 4, 6],\n",
    "    milestone_every=10,\n",
    "    save_dir=\"./\",\n",
    "    plot=True,\n",
    "    save=True,\n",
    "    test_imgs=test_imgs_noisy,\n",
    ")\n",
    "# trainer.plot_losses()\n",
    "# trainer.plot_reconstruction(imgs_256, samples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test visualize and evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_files = list((root/\"test_data\").glob(\"*.pt\"))\n",
    "test_data = torch.stack([normalizeAndResize(torch.load(f)) for f in tqdm(all_test_files[:10])]).view(-1, 256, 256)\n",
    "test_data = test_data[::10]\n",
    "print(f\"loaded {len(test_data):,.0f} test images with shape {test_data.shape[1:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_loss_and_reconstruction(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test_model(test_data, metrics=['mse','psnr']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "test_data_noisy = torch.stack([setExposure(s) for s in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_loss_and_reconstruction(test_imgs_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test_model(test_data_noisy, metrics=['mse','psnr']).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
